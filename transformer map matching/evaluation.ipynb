{
 "cells": [
  {
   "cell_type": "raw",
   "id": "4353d2ed",
   "metadata": {},
   "source": [
    "PREDICT ->  ['4455', '4455', '4455', '4455', '4455', '3691', '3707', '2905', '2916', '3729', '2903', '3740', '2902', '1173', '2898', '2896', '2895', '3812', '3830', '2893', '3847', '2892', '2911', '458', '2873', '2875', '768', '769', '770', '772', '773', '777', '778', '779', '781', '740', '741', '742', '742', '743', '2755', '744', '746', '746', '748', '750', '750', '751', '706', '707', '708', '709', '2765', '739', '705', '2756', '699', '699', '4234', '700', '701', '4233', '311', '702', '702', '703', '4390', '312', '4232', '4231', '753', '754', '755', '4230', '756', '757', '4229', '758', '759', '2891', '763', '2890']\n",
    "REAL    ->  ['4455', '4455', '4455', '4455', '4455', '2905', '2916', '3718', '2906', '3740', '2898', '3760', '2897', '3778', '2896', '3799', '3812', '3830', '2893', '3847', '2892', '1167', '1161', '1794', '2873', '764', '768', '769', '771', '772', '773', '777', '778', '780', '780', '781', '740', '741', '742', '742', '2755', '744', '746', '746', '748', '750', '750', '751', '706', '709', '2783', '732', '730', '739', '705', '2756', '699', '699', '4234', '700', '701', '4233', '311', '702', '703', '703', '696', '312', '1554', '4231', '753', '754', '755', '4230', '4330', '757', '4229', '759', '2890', '2598', '2878', '4361']\n",
    "预测路径边数 ->  113\n",
    "真实路径边数 ->  117\n",
    "重叠路径长度 ->  113\n",
    "重叠率 = 重叠路径长度 / max( 预测路径边数 , 真实路径边数 )  = 96.581% \n",
    "---\n",
    "/tmp/ipykernel_10752/1985595343.py:21: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
    "  edge_gap = edges[edges['start_node']==start_node ][ edges['end_node']==end_node ]['eid'].item()\n",
    "PREDICT ->  ['1632', '1665', '1667', '2291', '2298', '1660', '1626', '1651', '1645', '1674', '3349', '1677', '3012', '3014', '3344', '3010', '3338', '3352', '2321', '3018', '1573', '1598', '1585', '1581', '1602', '1604', '3356', '1687', '1685', '3360', '3023', '3362', '4421', '3364', '3367', '3369', '3371', '3373', '3377', '3029', '3381', '3383', '3033', '3385', '2219', '3391', '3392', '3393', '2178', '274', '1204']\n",
    "REAL    ->  ['1632', '1632', '1667', '2291', '1669', '1626', '1651', '1645', '1662', '2316', '1677', '3347', '3014', '1681', '3010', '3016', '3352', '2923', '3354', '2321', '1573', '1585', '1581', '1612', '1610', '1587', '3020', '1687', '3360', '3023', '3362', '4421', '3364', '3367', '3025', '3027', '3373', '3375', '3377', '3029', '3381', '3383', '3033', '3385', '2219', '3389', '3392', '3393', '3034', '274', '1204']\n",
    "预测路径边数 ->  80\n",
    "真实路径边数 ->  80\n",
    "重叠路径长度 ->  80\n",
    "重叠率 = 重叠路径长度 / max( 预测路径边数 , 真实路径边数 )  = 100.000% \n",
    "---\n",
    "PREDICT ->  ['1336', '1336', '3476', '623', '3478', '625', '3480', '627', '3483', '631', '3465']\n",
    "REAL    ->  ['3476', '3476', '623', '3478', '625', '3463', '617', '3581', '3468', '3472', '619']\n",
    "预测路径边数 ->  12\n",
    "真实路径边数 ->  17\n",
    "重叠路径长度 ->  11\n",
    "重叠率 = 重叠路径长度 / max( 预测路径边数 , 真实路径边数 )  = 64.706% \n",
    "---\n",
    "PREDICT ->  ['4453', '4453', '4453', '4456', '2153', '4049', '2160', '2151', '2149', '1186', '2142', '2145', '2146', '4487', '2148', '4488', '4489', '4490', '4491', '1187', '4492', '2182', '4494', '4495', '2186', '2187', '2190', '4501', '1183', '4502', '2206', '2238', '2204', '2189', '2202', '1181', '264', '2174', '2159', '1176', '2045']\n",
    "REAL    ->  ['1190', '4453', '4453', '4456', '2153', '4049', '2160', '2151', '2149', '1186', '2146', '4487', '2148', '4488', '4489', '4490', '4491', '1187', '2181', '2182', '2183', '2184', '4496', '4497', '2187', '4499', '4500', '4501', '1183', '4484', '2193', '2100', '261', '2159', '1176', '1174', '2045', '1175', '2202', '1181', '264']\n",
    "预测路径边数 ->  60\n",
    "真实路径边数 ->  67\n",
    "重叠路径长度 ->  46\n",
    "重叠率 = 重叠路径长度 / max( 预测路径边数 , 真实路径边数 )  = 68.657% \n",
    "---\n",
    "PREDICT ->  ['4183', '2611', '4207', '4181', '2836', '2638', '1951', '2388', '2781', '2766', '2647', '1957', '2306', '2651', '2747', '2725', '2709', '2645', '2641', '2683', '2683', '2671', '2659', '2953', '2643', '2629', '2603', '2587', '2947', '2943', '2545']\n",
    "REAL    ->  ['1949', '3002', '4185', '2610', '2836', '2638', '1951', '2388', '2800', '2781', '2647', '1957', '2306', '2747', '2649', '2725', '2697', '2645', '2641', '2683', '2683', '2671', '2659', '2953', '2951', '2629', '2617', '2587', '2570', '2947', '2945']\n",
    "预测路径边数 ->  41\n",
    "真实路径边数 ->  44\n",
    "重叠路径长度 ->  36\n",
    "重叠率 = 重叠路径长度 / max( 预测路径边数 , 真实路径边数 )  = 81.818% \n",
    "---\n",
    "PREDICT ->  ['4177', '4221', '4235', '4282', '4294', '4307', '4325']\n",
    "REAL    ->  ['4177', '4177', '4269', '4282', '4294', '4307', '4325']\n",
    "预测路径边数 ->  10\n",
    "真实路径边数 ->  10\n",
    "重叠路径长度 ->  10\n",
    "重叠率 = 重叠路径长度 / max( 预测路径边数 , 真实路径边数 )  = 100.000% \n",
    "---\n",
    "PREDICT ->  ['3969', '3969', '1621', '3433', '3116', '1634', '3115', '1647', '1650', '1656', '1625', '1640', '2294', '1664', '3008', '3012', '3347', '3014', '3344', '3010', '3340', '3352', '2923', '3354', '2321', '3018', '1573', '1598', '1585', '1602', '1589', '1612', '1610', '1587', '1604']\n",
    "REAL    ->  ['3118', '3969', '1621', '3433', '3116', '1634', '1642', '3115', '1647', '3114', '1656', '2294', '1664', '1662', '2316', '3347', '3014', '1681', '3344', '3342', '3340', '3352', '3354', '2321', '1573', '1570', '1595', '1613', '1578', '1574', '1576', '1609', '1588', '1601', '1584']\n",
    "预测路径边数 ->  47\n",
    "真实路径边数 ->  57\n",
    "重叠路径长度 ->  38\n",
    "重叠率 = 重叠路径长度 / max( 预测路径边数 , 真实路径边数 )  = 66.667% \n",
    "---\n",
    "PREDICT ->  ['1126', '39', '35', '1116', '30', '1114', '28', '1110', '2465', '4190', '4187', '1106', '2468', '2478', '1104', '2479', '2480', '1101', '2481', '4192', '2482', '4193', '2483', '2486', '4134', '1088', '4135', '1095', '4137', '4138', '2023', '2022', '4155', '2501', '24', '2499', '3643', '1073', '1070', '2496', '2494', '1071', '1074', '1076', '116', '117', '4328', '2502', '118', '4329', '1090', '2504', '1096', '2041', '2510', '4122', '19', '4126', '1082', '2525', '4128', '2527', '1080']\n",
    "REAL    ->  ['41', '39', '37', '1116', '30', '1114', '28', '26', '2465', '4190', '4187', '2467', '2468', '2478', '2479', '2480', '1101', '2481', '4192', '2482', '4193', '1089', '2486', '1086', '1087', '1088', '1091', '2537', '4137', '2024', '2023', '2022', '4155', '2501', '24', '2499', '3643', '1070', '2496', '4317', '2494', '1071', '1074', '1079', '117', '4327', '4328', '118', '4329', '1090', '119', '2504', '1096', '2041', '4320', '4122', '19', '1082', '2525', '4128', '2531', '2533', '4132']\n",
    "预测路径边数 ->  98\n",
    "真实路径边数 ->  101\n",
    "重叠路径长度 ->  97\n",
    "重叠率 = 重叠路径长度 / max( 预测路径边数 , 真实路径边数 )  = 96.040% \n",
    "---\n",
    "PREDICT ->  ['1559', '304', '2591', '1561', '2991', '1691', '1689', '1689', '4251', '85', '1250', '84']\n",
    "REAL    ->  ['2993', '2993', '304', '1561', '2991', '1691', '1689', '4249', '4251', '1250', '84', '82']\n",
    "预测路径边数 ->  15\n",
    "真实路径边数 ->  17\n",
    "重叠路径长度 ->  15\n",
    "重叠率 = 重叠路径长度 / max( 预测路径边数 , 真实路径边数 )  = 88.235% \n",
    "---\n",
    "PREDICT ->  ['1230', '1243', '1260']\n",
    "REAL    ->  ['1230', '1230', '1260']\n",
    "预测路径边数 ->  3\n",
    "真实路径边数 ->  3\n",
    "重叠路径长度 ->  3\n",
    "重叠率 = 重叠路径长度 / max( 预测路径边数 , 真实路径边数 )  = 100.000% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53119200",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture #忽略输出\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "import Ipynb_importer\n",
    "\n",
    "import s07_训练数据生成 as datagen\n",
    "\n",
    "edges, nodes = datagen.load_data(\"edgeOSM.txt\",\"nodeOSM.txt\")\n",
    "G = datagen.create_networkx_graph(edges, nodes)\n",
    "\n",
    "def gen_full_edges(edge_ids):\n",
    "    full_edges = []\n",
    "\n",
    "    last_end_node_id = None\n",
    "    last_edge_id = None\n",
    "\n",
    "    for eid in edge_ids:\n",
    "        eid = int(eid)\n",
    "        if last_edge_id:\n",
    "            if eid == last_edge_id :\n",
    "                continue\n",
    "        if last_end_node_id:\n",
    "            curr_start_node_id = edges.loc[eid,'start_node']\n",
    "            node_path = nx.shortest_path(G, source=last_end_node_id, target=curr_start_node_id)\n",
    "            if len(node_path) >1 :\n",
    "                for i in range(1,len(node_path)):\n",
    "                    start_node = node_path[i-1]\n",
    "                    end_node = node_path[i]\n",
    "                    edge_gap = edges[edges['start_node']==start_node ][ edges['end_node']==end_node ]['eid'].item()\n",
    "                    full_edges += [edge_gap]\n",
    "        full_edges += [eid]\n",
    "        last_end_node_id = edges.loc[eid,'end_node']\n",
    "        last_edge_id = eid\n",
    "    return full_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3486554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'reading', 'books']\n",
      "['I', 'enjoy', 'reading', 'books']\n",
      "BELU分数:  0.3125\n"
     ]
    }
   ],
   "source": [
    "import nltk.translate.bleu_score as bleu\n",
    "\n",
    "# 参考文本和翻译结果\n",
    "ref_text = 'I love reading books'\n",
    "hyp_text = 'I enjoy reading books'\n",
    "\n",
    "# 分词\n",
    "ref_tokens = ref_text.split()\n",
    "hyp_tokens = hyp_text.split()\n",
    "\n",
    "print(ref_tokens)\n",
    "print(hyp_tokens)\n",
    "\n",
    "# 计算BELU分数\n",
    "bleu_1 = bleu.sentence_bleu([ref_tokens], hyp_tokens, weights=(1, 0, 0, 0))\n",
    "bleu_2 = bleu.sentence_bleu([ref_tokens], hyp_tokens, weights=(0.5, 0.5, 0, 0))\n",
    "bleu_3 = bleu.sentence_bleu([ref_tokens], hyp_tokens, weights=(0.33, 0.33, 0.33, 0))\n",
    "bleu_4 = bleu.sentence_bleu([ref_tokens], hyp_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "belu_score = (bleu_1 + bleu_2 + bleu_3 + bleu_4) / 4\n",
    "\n",
    "# 输出结果\n",
    "print(\"BELU分数: \", belu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886e3cc0",
   "metadata": {},
   "source": [
    "上述代码中使用nltk库中的`bleu_score`模块计算BELU分数，对于一个文本样本，计算出BLEU-1、BLEU-2、BLEU-3和BLEU-4的分数，并对其作平均，得到最终的BELU分数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "237e4d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: [1.  1.  0.  0.5 1. ]\n",
      "Recall: [1. 1. 0. 1. 1.]\n",
      "F-score: [1.         1.         0.         0.66666667 1.        ]\n",
      "Support: [1 1 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "y_true = [1, 2, 3, 4, 5]\n",
    "y_pred = [1, 2, 4, 4, 5]\n",
    "\n",
    "precision, recall, fscore, support = score(y_true, y_pred)\n",
    "\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F-score:', fscore)\n",
    "print('Support:', support)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536dc684",
   "metadata": {},
   "source": [
    "这里我们使用了`sklearn.metrics`中的`precision_recall_fscore_support`进行计算precision、recall和F-score。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dc84ced1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICT =  ['1559', '304', '2591', '1561', '2991', '1691', '1689', '1689', '4251', '85', '1250', '84']\n",
    "REAL    =['2993', '2993', '304', '1561', '2991', '1691', '1689', '4249', '4251', '1250', '84', '82']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9c3b7d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1559, 304, 4253, 2591, 2989, 1563, 1561, 2991, 1691, 1689, 4249, 4251, 85, 1250, 84]\n",
      "[2993, 1559, 304, 4253, 2591, 2989, 1563, 1561, 2991, 1691, 1689, 4249, 4251, 85, 1250, 84, 82]\n",
      "BELU分数:  0.8751733190429475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\4\\ipykernel_6624\\399580839.py:29: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  edge_gap = edges[edges['start_node']==start_node ][ edges['end_node']==end_node ]['eid'].item()\n"
     ]
    }
   ],
   "source": [
    "# 分词\n",
    "predict = gen_full_edges(PREDICT)\n",
    "real = gen_full_edges(REAL)\n",
    "\n",
    "\n",
    "ref_tokens = gen_full_edges(real)\n",
    "hyp_tokens = gen_full_edges(predict)\n",
    "\n",
    "print(hyp_tokens)\n",
    "print(ref_tokens)\n",
    "\n",
    "\n",
    "# 计算BELU分数\n",
    "bleu_1 = bleu.sentence_bleu([ref_tokens], hyp_tokens, weights=(1, 0, 0, 0))\n",
    "bleu_2 = bleu.sentence_bleu([ref_tokens], hyp_tokens, weights=(0.5, 0.5, 0, 0))\n",
    "bleu_3 = bleu.sentence_bleu([ref_tokens], hyp_tokens, weights=(0.33, 0.33, 0.33, 0))\n",
    "bleu_4 = bleu.sentence_bleu([ref_tokens], hyp_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "belu_score = (bleu_1 + bleu_2 + bleu_3 + bleu_4) / 4\n",
    "\n",
    "print(\"BELU分数: \", belu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "db562c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一个相同的数字为 1559 ，位置分别为 1 0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 score: 1.0\n",
      "F2 score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "list1 = real\n",
    "list2 = predict\n",
    "\n",
    "\n",
    "for i in range(len(list1)):\n",
    "    for j in range(len(list2)):\n",
    "        if list1[i] == list2[j]:\n",
    "            print(\"第一个相同的数字为\", list1[i], \"，位置分别为\", i, j)\n",
    "            break\n",
    "    else:\n",
    "        continue\n",
    "    break\n",
    "\n",
    "list1 = list1[i:]\n",
    "list2 = list2[j:]\n",
    "    \n",
    "    \n",
    "# 求最短的序列长度\n",
    "length = min(len(list1), len(list2))\n",
    "\n",
    "# 切片截断序列\n",
    "real = list1[:length]\n",
    "predict = list2[:length]\n",
    "\n",
    "\n",
    "precision = precision_score(real,predict,average='micro')\n",
    "\n",
    "recall = recall_score(real,predict,average='micro')\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "def f_score(precision, recall, beta=1):\n",
    "    beta_squared = beta ** 2\n",
    "    numerator = (1 + beta_squared) * precision * recall\n",
    "    denominator = beta_squared * precision + recall\n",
    "    return numerator / denominator\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 计算 F1 score\n",
    "f1_score = f_score(precision, recall)\n",
    "print(\"F1 score:\", f1_score)\n",
    "\n",
    "# 计算 F2 score\n",
    "f2_score = f_score(precision, recall, beta=2)\n",
    "print(\"F2 score:\", f2_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
